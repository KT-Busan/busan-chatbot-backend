import requests
import re
import time
import json
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime


class BusanYouthProgramCrawler:
    def __init__(self):
        self.base_url = "https://young.busan.go.kr"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.programs_data = []

    def get_page_content(self, url, encoding='utf-8'):
        """í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            print(f"í˜ì´ì§€ ìš”ì²­: {url}")
            response = self.session.get(url, timeout=15)
            response.encoding = encoding

            if response.status_code == 200:
                return BeautifulSoup(response.content, 'html.parser')
            else:
                print(f"HTTP ì˜¤ë¥˜: {response.status_code}")
                return None

        except Exception as e:
            print(f"í˜ì´ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None

    def extract_program_info_from_li(self, li_element):
        """li ìš”ì†Œì—ì„œ í”„ë¡œê·¸ë¨ ì •ë³´ ì¶”ì¶œ"""
        try:
            program_info = {
                'title': '',
                'region': '',
                'location': '',
                'status': '',
                'application_period': '',
                'link': '',
                'program_date': '',
                'description': ''
            }

            # ë§í¬ ì¶”ì¶œ
            link_element = li_element.select_one('a')
            if link_element:
                href = link_element.get('href', '')
                if href:
                    program_info['link'] = urljoin(self.base_url, href)

            # ëª¨ì§‘ ìƒíƒœ í™•ì¸ (ëª¨ì§‘ì¤‘ì¸ ê²ƒë§Œ)
            recruit_state = li_element.select_one('.recruit_state .ing')
            if not recruit_state or recruit_state.get_text(strip=True) != 'ëª¨ì§‘ì¤‘':
                return None  # ëª¨ì§‘ì¤‘ì´ ì•„ë‹ˆë©´ ì œì™¸

            program_info['status'] = 'ëª¨ì§‘ì¤‘'

            # ì œëª© ì¶”ì¶œ
            recruit_tit = li_element.select_one('.recruit_tit')
            if recruit_tit:
                title = recruit_tit.get_text(strip=True)
                program_info['title'] = title

                # ì œëª©ì—ì„œ ì§€ì—­ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: [í•´ìš´ëŒ€êµ¬], [ê¸ˆì •êµ¬])
                region_match = re.search(r'\[([^]]+êµ¬)\]', title)
                if region_match:
                    program_info['region'] = region_match.group(1)

            # ì‹ ì²­ê¸°ê°„ ì¶”ì¶œ
            recruit_date = li_element.select_one('.recruit_date')
            if recruit_date:
                date_spans = recruit_date.find_all('span')
                if len(date_spans) >= 2:
                    date_text = date_spans[1].get_text(strip=True)
                    program_info['application_period'] = date_text

            # ì¥ì†Œ/ê¸°ê´€ ì •ë³´ ì¶”ì¶œ (part3ì—ì„œ)
            part3 = li_element.select_one('.part3')
            if part3:
                location_text = part3.get_text(strip=True)
                if location_text:
                    program_info['location'] = location_text

            # ìµœì†Œ ì¡°ê±´ í™•ì¸ (ì œëª©ê³¼ ëª¨ì§‘ì¤‘ ìƒíƒœê°€ ìˆì–´ì•¼ í•¨)
            if program_info['title'] and program_info['status'] == 'ëª¨ì§‘ì¤‘':
                return program_info
            else:
                return None

        except Exception as e:
            print(f"í”„ë¡œê·¸ë¨ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None

    def extract_programs_from_page(self, soup):
        """í˜ì´ì§€ì—ì„œ í”„ë¡œê·¸ë¨ ëª©ë¡ ì¶”ì¶œ"""
        programs = []

        # ul > li êµ¬ì¡°ì—ì„œ í”„ë¡œê·¸ë¨ ì¶”ì¶œ
        program_list = soup.select('ul li')

        for li_element in program_list:
            try:
                # recruit_stateê°€ ìˆëŠ” lië§Œ ì²˜ë¦¬ (í”„ë¡œê·¸ë¨ í•­ëª©)
                if li_element.select_one('.recruit_state'):
                    program_info = self.extract_program_info_from_li(li_element)
                    if program_info:
                        programs.append(program_info)
            except Exception as e:
                continue

        return programs

    def has_program_content(self, soup):
        """í˜ì´ì§€ì— í”„ë¡œê·¸ë¨ ì½˜í…ì¸ ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        if not soup:
            return False
        program_items = soup.select('.recruit_state')
        return len(program_items) > 0

    def crawl_all_programs(self):
        """ëª¨ë“  ì²­ë…„ í”„ë¡œê·¸ë¨ í¬ë¡¤ë§"""
        print("ë¶€ì‚° ì²­ë…„ í”„ë¡œê·¸ë¨ í¬ë¡¤ë§ ì‹œì‘")
        all_programs = []

        # ì—¬ëŸ¬ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œë„
        for page in range(1, 6):  # ìµœëŒ€ 5í˜ì´ì§€ê¹Œì§€
            print(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")

            if page == 1:
                url = "https://young.busan.go.kr/policySupport/act.nm?menuCd=261"
            else:
                # ë‹¤ì–‘í•œ í˜ì´ì§€ë„¤ì´ì…˜ URL ì‹œë„
                possible_urls = [
                    f"https://young.busan.go.kr/policySupport/act.nm?menuCd=261&pageIndex={page}",
                    f"https://young.busan.go.kr/policySupport/act.nm?menuCd=261&page={page}",
                    f"https://young.busan.go.kr/policySupport/act.nm?menuCd=261&currentPage={page}",
                ]

                soup = None
                for url in possible_urls:
                    soup = self.get_page_content(url)
                    if soup and self.has_program_content(soup):
                        break
                    time.sleep(0.5)

                if not soup or not self.has_program_content(soup):
                    print(f"í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ í”„ë¡œê·¸ë¨ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break

            if page == 1:
                soup = self.get_page_content(url)

            if not soup:
                continue

            page_programs = self.extract_programs_from_page(soup)
            if not page_programs:  # í”„ë¡œê·¸ë¨ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
                print(f"í˜ì´ì§€ {page}ì—ì„œ í”„ë¡œê·¸ë¨ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break

            all_programs.extend(page_programs)
            print(f"í˜ì´ì§€ {page}ì—ì„œ {len(page_programs)}ê°œ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘")
            time.sleep(1)  # í˜ì´ì§€ ê°„ ì§€ì—°

        print(f"í¬ë¡¤ë§ ì™„ë£Œ: {len(all_programs)}ê°œ ëª¨ì§‘ì¤‘ì¸ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘")
        self.programs_data = all_programs
        return all_programs


def get_youth_programs_data():
    """ì²­ë…„ í”„ë¡œê·¸ë¨ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    import os
    from datetime import datetime, timedelta

    cache_file = 'youth_programs_cache.json'
    cache_duration = timedelta(hours=6)  # 6ì‹œê°„ ìºì‹œ (í”„ë¡œê·¸ë¨ì€ ë” ìì£¼ ì—…ë°ì´íŠ¸)

    # ìºì‹œ íŒŒì¼ í™•ì¸
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)

            cache_time = datetime.fromisoformat(cached_data['cached_at'])
            if datetime.now() - cache_time < cache_duration:
                print("ìºì‹œëœ í”„ë¡œê·¸ë¨ ë°ì´í„° ì‚¬ìš©")
                return cached_data['data']
        except:
            pass

    # ìƒˆë¡œ í¬ë¡¤ë§
    print("ğŸ”„ ìƒˆë¡œìš´ í”„ë¡œê·¸ë¨ ë°ì´í„° í¬ë¡¤ë§ ì¤‘...")
    crawler = BusanYouthProgramCrawler()
    programs = crawler.crawl_all_programs()

    # ìºì‹œ ì €ì¥
    cache_data = {
        'cached_at': datetime.now().isoformat(),
        'data': programs
    }

    try:
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        print("í”„ë¡œê·¸ë¨ ìºì‹œ ì €ì¥ ì™„ë£Œ")
    except:
        pass

    return programs


def search_programs_by_region(region):
    """ì§€ì—­ë³„ ì²­ë…„ í”„ë¡œê·¸ë¨ ê²€ìƒ‰"""
    programs = get_youth_programs_data()
    if not programs:
        return "í˜„ì¬ ì²­ë…„ í”„ë¡œê·¸ë¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ì§€ì—­ëª… ì •ê·œí™” (êµ¬ ì œê±°)
    region_normalized = region.replace('êµ¬', '') if region.endswith('êµ¬') else region

    filtered_programs = []
    for program in programs:
        program_region = program.get('region', '')
        program_location = program.get('location', '')
        program_title = program.get('title', '')

        if (region_normalized in program_region or
                region_normalized in program_location or
                region_normalized in program_title):
            filtered_programs.append(program)

    if not filtered_programs:
        return f"**{region}**ì—ì„œ í˜„ì¬ ëª¨ì§‘ì¤‘ì¸ ì²­ë…„ í”„ë¡œê·¸ë¨ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\në‹¤ë¥¸ ì§€ì—­ì„ ê²€ìƒ‰í•´ë³´ì„¸ìš”!"

    result = f"**{region} ì²­ë…„ í”„ë¡œê·¸ë¨** ({len(filtered_programs)}ê°œ ëª¨ì§‘ì¤‘)\n\n"

    for program in filtered_programs[:8]:  # ìµœëŒ€ 8ê°œ í‘œì‹œ
        result += format_program_info(program) + "\n"

    if len(filtered_programs) > 8:
        result += f"\n... ì™¸ {len(filtered_programs) - 8}ê°œ í”„ë¡œê·¸ë¨ ë” ìˆìŒ"

    return result


def search_programs_by_keyword(keyword):
    """í‚¤ì›Œë“œë³„ ì²­ë…„ í”„ë¡œê·¸ë¨ ê²€ìƒ‰"""
    programs = get_youth_programs_data()
    if not programs:
        return "í˜„ì¬ ì²­ë…„ í”„ë¡œê·¸ë¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    filtered_programs = []
    keyword_lower = keyword.lower()

    for program in programs:
        if (keyword_lower in program.get('title', '').lower() or
                keyword_lower in program.get('location', '').lower() or
                keyword_lower in program.get('region', '').lower()):
            filtered_programs.append(program)

    if not filtered_programs:
        return f"**{keyword}** ê´€ë ¨ ëª¨ì§‘ì¤‘ì¸ ì²­ë…„ í”„ë¡œê·¸ë¨ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\në‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”!"

    result = f"ğŸ” **{keyword}** ê²€ìƒ‰ ê²°ê³¼ ({len(filtered_programs)}ê°œ ëª¨ì§‘ì¤‘)\n\n"

    for program in filtered_programs[:8]:  # ìµœëŒ€ 8ê°œ í‘œì‹œ
        result += format_program_info(program) + "\n"

    if len(filtered_programs) > 8:
        result += f"\n... ì™¸ {len(filtered_programs) - 8}ê°œ í”„ë¡œê·¸ë¨ ë” ìˆìŒ"

    return result


def format_program_info(program):
    """í”„ë¡œê·¸ë¨ ì •ë³´ í¬ë§·íŒ…"""
    result = f"**{program['title']}**\n"

    if program.get('status'):
        status_emoji = "ğŸŸ¢" if program['status'] == 'ëª¨ì§‘ì¤‘' else "ğŸ”´"
        result += f"{status_emoji} {program['status']}\n"

    if program.get('application_period'):
        result += f"ğŸ“… ì‹ ì²­ê¸°ê°„: {program['application_period']}\n"

    if program.get('location'):
        result += f"ğŸ“ ì¥ì†Œ: {program['location']}\n"

    if program.get('region'):
        result += f"ğŸ›ï¸ ì§€ì—­: {program['region']}\n"

    if program.get('link'):
        result += f"ğŸ”— [ìì„¸íˆ ë³´ê¸°]({program['link']})\n"

    return result


def get_all_youth_programs():
    """ì „ì²´ ëª¨ì§‘ì¤‘ì¸ ì²­ë…„ í”„ë¡œê·¸ë¨ ëª©ë¡"""
    programs = get_youth_programs_data()
    if not programs:
        return "í˜„ì¬ ì²­ë…„ í”„ë¡œê·¸ë¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    result = f"**ë¶€ì‚° ì²­ë…„ í”„ë¡œê·¸ë¨ ëª¨ì§‘ì¤‘** ({len(programs)}ê°œ)\n\n"

    # ì§€ì—­ë³„ë¡œ ê·¸ë£¹í™”
    regions = {}
    for program in programs:
        region = program.get('region', 'ê¸°íƒ€')
        if not region or region == 'ê¸°íƒ€':
            # ì œëª©ì—ì„œ ì§€ì—­ ì¶”ì¶œ ì‹œë„
            title = program.get('title', '')
            region_match = re.search(r'\[([^]]+êµ¬)\]', title)
            if region_match:
                region = region_match.group(1)
            else:
                region = 'ì „ì²´/ê¸°íƒ€'

        if region not in regions:
            regions[region] = []
        regions[region].append(program)

    for region, region_programs in sorted(regions.items()):
        result += f"**{region}** ({len(region_programs)}ê°œ)\n"
        for program in region_programs[:3]:  # ê° ì§€ì—­ë‹¹ ìµœëŒ€ 3ê°œ
            result += f"{program['title']}\n"
            if program.get('application_period'):
                result += f"{program['application_period']}\n"

        if len(region_programs) > 3:
            result += f"     ... ì™¸ {len(region_programs) - 3}ê°œ ë”\n"
        result += "\n"

    result += "ğŸ’¡ ì§€ì—­ëª…ì´ë‚˜ í”„ë¡œê·¸ë¨ëª…ìœ¼ë¡œ ìì„¸í•œ ì •ë³´ë¥¼ ê²€ìƒ‰í•´ë³´ì„¸ìš”!"
    return result


def get_programs_by_category():
    """ì¹´í…Œê³ ë¦¬ë³„ í”„ë¡œê·¸ë¨ ë¶„ë¥˜"""
    programs = get_youth_programs_data()
    if not programs:
        return "í˜„ì¬ ì²­ë…„ í”„ë¡œê·¸ë¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    categories = {
        'ì·¨ì—…/ì§„ë¡œ': [],
        'êµìœ¡/ê°•ì˜': [],
        'ì°½ì—…': [],
        'ë¬¸í™”/ì˜ˆìˆ ': [],
        'ê¸°íƒ€': []
    }

    for program in programs:
        title = program.get('title', '').lower()
        categorized = False

        if any(keyword in title for keyword in ['ì·¨ì—…', 'job', 'ì»¨ì„¤íŒ…', 'ë©´ì ‘', 'ì´ë ¥ì„œ']):
            categories['ì·¨ì—…/ì§„ë¡œ'].append(program)
            categorized = True
        elif any(keyword in title for keyword in ['êµìœ¡', 'ê°•ì˜', 'ê³¼ì •', 'êµì‹¤', 'ìŠ¤ì¿¨']):
            categories['êµìœ¡/ê°•ì˜'].append(program)
            categorized = True
        elif any(keyword in title for keyword in ['ì°½ì—…', 'ì‚¬ì—…', 'ë¹„ì¦ˆë‹ˆìŠ¤']):
            categories['ì°½ì—…'].append(program)
            categorized = True
        elif any(keyword in title for keyword in ['ë¬¸í™”', 'ì˜ˆìˆ ', 'ê³µì—°', 'ì „ì‹œ', 'ìŒì•…', 'ë¯¸ìˆ ']):
            categories['ë¬¸í™”/ì˜ˆìˆ '].append(program)
            categorized = True

        if not categorized:
            categories['ê¸°íƒ€'].append(program)

    result = "**ì¹´í…Œê³ ë¦¬ë³„ ì²­ë…„ í”„ë¡œê·¸ë¨**\n\n"

    for category, category_programs in categories.items():
        if category_programs:
            result += f"**{category}** ({len(category_programs)}ê°œ)\n"
            for program in category_programs[:3]:
                result += f"{program['title']}\n"
            if len(category_programs) > 3:
                result += f"     ... ì™¸ {len(category_programs) - 3}ê°œ ë”\n"
            result += "\n"

    return result


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
if __name__ == "__main__":
    # í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
    crawler = BusanYouthProgramCrawler()
    programs = crawler.crawl_all_programs()

    print(f"\nì´ {len(programs)}ê°œ ëª¨ì§‘ì¤‘ì¸ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘:")
    for i, program in enumerate(programs[:5], 1):
        print(f"\n{i}. {program['title']}")
        print(f"   ìƒíƒœ: {program['status']}")
        print(f"   ì‹ ì²­ê¸°ê°„: {program['application_period']}")
        print(f"   ì¥ì†Œ: {program['location']}")
        if program['link']:
            print(f"   ë§í¬: {program['link']}")