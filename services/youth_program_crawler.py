import requests
import re
import time
import json
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime


class BusanYouthProgramCrawler:
    def __init__(self):
        self.base_url = "https://young.busan.go.kr"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.programs_data = []

    def get_page_content(self, url, encoding='utf-8'):
        """í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            print(f"í˜ì´ì§€ ìš”ì²­: {url}")
            response = self.session.get(url, timeout=15)
            response.encoding = encoding

            if response.status_code == 200:
                return BeautifulSoup(response.content, 'html.parser')
            else:
                print(f"HTTP ì˜¤ë¥˜: {response.status_code}")
                return None

        except Exception as e:
            print(f"í˜ì´ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None

    def extract_program_info_from_li(self, li_element):
        """li ìš”ì†Œì—ì„œ í”„ë¡œê·¸ë¨ ì •ë³´ ì¶”ì¶œ"""
        try:
            program_info = {
                'title': '',
                'region': '',
                'location': '',
                'status': '',
                'application_period': '',
                'link': '',
                'program_date': '',
                'description': ''
            }

            # ë§í¬ ì¶”ì¶œ
            link_element = li_element.select_one('a')
            if link_element:
                href = link_element.get('href', '')
                if href:
                    program_info['link'] = urljoin(self.base_url, href)

            # ëª¨ì§‘ ìƒíƒœ í™•ì¸ (ëª¨ì§‘ì¤‘ì¸ ê²ƒë§Œ)
            recruit_state = li_element.select_one('.recruit_state .ing')
            if not recruit_state or recruit_state.get_text(strip=True) != 'ëª¨ì§‘ì¤‘':
                return None  # ëª¨ì§‘ì¤‘ì´ ì•„ë‹ˆë©´ ì œì™¸

            program_info['status'] = 'ëª¨ì§‘ì¤‘'

            # ì œëª© ì¶”ì¶œ
            recruit_tit = li_element.select_one('.recruit_tit')
            if recruit_tit:
                title = recruit_tit.get_text(strip=True)
                program_info['title'] = title

                # ì œëª©ì—ì„œ ì§€ì—­ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: [í•´ìš´ëŒ€êµ¬], [ê¸ˆì •êµ¬])
                region_match = re.search(r'\[([^]]+êµ¬)\]', title)
                if region_match:
                    program_info['region'] = region_match.group(1)

            # ì‹ ì²­ê¸°ê°„ ì¶”ì¶œ
            recruit_date = li_element.select_one('.recruit_date')
            if recruit_date:
                date_spans = recruit_date.find_all('span')
                if len(date_spans) >= 2:
                    date_text = date_spans[1].get_text(strip=True)
                    program_info['application_period'] = date_text

            # ì¥ì†Œ/ê¸°ê´€ ì •ë³´ ì¶”ì¶œ (part3ì—ì„œ)
            part3 = li_element.select_one('.part3')
            if part3:
                location_text = part3.get_text(strip=True)
                if location_text:
                    program_info['location'] = location_text

            # ìµœì†Œ ì¡°ê±´ í™•ì¸ (ì œëª©ê³¼ ëª¨ì§‘ì¤‘ ìƒíƒœê°€ ìˆì–´ì•¼ í•¨)
            if program_info['title'] and program_info['status'] == 'ëª¨ì§‘ì¤‘':
                return program_info
            else:
                return None

        except Exception as e:
            print(f"í”„ë¡œê·¸ë¨ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None

    def extract_programs_from_page(self, soup):
        """í˜ì´ì§€ì—ì„œ í”„ë¡œê·¸ë¨ ëª©ë¡ ì¶”ì¶œ"""
        programs = []

        # ul > li êµ¬ì¡°ì—ì„œ í”„ë¡œê·¸ë¨ ì¶”ì¶œ
        program_list = soup.select('ul li')

        for li_element in program_list:
            try:
                # recruit_stateê°€ ìˆëŠ” lië§Œ ì²˜ë¦¬ (í”„ë¡œê·¸ë¨ í•­ëª©)
                if li_element.select_one('.recruit_state'):
                    program_info = self.extract_program_info_from_li(li_element)
                    if program_info:
                        programs.append(program_info)
            except Exception as e:
                continue

        return programs

    def has_program_content(self, soup):
        """í˜ì´ì§€ì— í”„ë¡œê·¸ë¨ ì½˜í…ì¸ ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        if not soup:
            return False
        program_items = soup.select('.recruit_state')
        return len(program_items) > 0

    def crawl_all_programs(self):
        """ëª¨ë“  ì²­ë…„ í”„ë¡œê·¸ë¨ í¬ë¡¤ë§"""
        print("ë¶€ì‚° ì²­ë…„ í”„ë¡œê·¸ë¨ í¬ë¡¤ë§ ì‹œì‘")
        all_programs = []

        # ì—¬ëŸ¬ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œë„
        for page in range(1, 6):  # ìµœëŒ€ 5í˜ì´ì§€ê¹Œì§€
            print(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")

            if page == 1:
                url = "https://young.busan.go.kr/policySupport/act.nm?menuCd=261"
            else:
                # ë‹¤ì–‘í•œ í˜ì´ì§€ë„¤ì´ì…˜ URL ì‹œë„
                possible_urls = [
                    f"https://young.busan.go.kr/policySupport/act.nm?menuCd=261&pageIndex={page}",
                    f"https://young.busan.go.kr/policySupport/act.nm?menuCd=261&page={page}",
                    f"https://young.busan.go.kr/policySupport/act.nm?menuCd=261&currentPage={page}",
                ]

                soup = None
                for url in possible_urls:
                    soup = self.get_page_content(url)
                    if soup and self.has_program_content(soup):
                        break
                    time.sleep(0.5)

                if not soup or not self.has_program_content(soup):
                    print(f"í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ í”„ë¡œê·¸ë¨ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break

            if page == 1:
                soup = self.get_page_content(url)

            if not soup:
                continue

            page_programs = self.extract_programs_from_page(soup)
            if not page_programs:  # í”„ë¡œê·¸ë¨ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
                print(f"í˜ì´ì§€ {page}ì—ì„œ í”„ë¡œê·¸ë¨ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break

            all_programs.extend(page_programs)
            print(f"í˜ì´ì§€ {page}ì—ì„œ {len(page_programs)}ê°œ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘")
            time.sleep(1)  # í˜ì´ì§€ ê°„ ì§€ì—°

        print(f"í¬ë¡¤ë§ ì™„ë£Œ: {len(all_programs)}ê°œ ëª¨ì§‘ì¤‘ì¸ í”„ë¡œê·¸ë¨ ìˆ˜ì§‘")
        self.programs_data = all_programs
        return all_programs


def parse_deadline_date(application_period):
    """ì‹ ì²­ê¸°ê°„ì—ì„œ ë§ˆê°ì¼ ì¶”ì¶œ ë° íŒŒì‹±"""
    try:
        if not application_period:
            return None

        # "2024.12.01 ~ 2024.12.31" í˜•íƒœì—ì„œ ë§ˆê°ì¼ ì¶”ì¶œ
        import re
        date_pattern = r'(\d{4})[.-](\d{1,2})[.-](\d{1,2})'
        dates = re.findall(date_pattern, application_period)

        if len(dates) >= 2:
            # ë§ˆê°ì¼ (ë‘ ë²ˆì§¸ ë‚ ì§œ)
            year, month, day = dates[1]
            return datetime(int(year), int(month), int(day))
        elif len(dates) == 1:
            # ë‚ ì§œê°€ í•˜ë‚˜ë§Œ ìˆëŠ” ê²½ìš°
            year, month, day = dates[0]
            return datetime(int(year), int(month), int(day))

        return None
    except:
        return None


def get_youth_programs_data():
    """ì²­ë…„ í”„ë¡œê·¸ë¨ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    import os
    from datetime import datetime, timedelta

    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ instance í´ë” ê²½ë¡œ ì„¤ì •
    basedir = os.path.abspath(os.path.dirname(__file__))
    project_root = os.path.dirname(basedir)  # servicesì˜ ìƒìœ„ í´ë” (í”„ë¡œì íŠ¸ ë£¨íŠ¸)
    instance_path = os.path.join(os.environ.get('RENDER_DISK_PATH', project_root), 'instance')
    if not os.path.exists(instance_path):
        os.makedirs(instance_path)

    cache_file = os.path.join(instance_path, 'youth_programs_cache.json')
    cache_duration = timedelta(hours=6)  # 6ì‹œê°„ ìºì‹œ (í”„ë¡œê·¸ë¨ì€ ë” ìì£¼ ì—…ë°ì´íŠ¸)

    # ìºì‹œ íŒŒì¼ í™•ì¸
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)

            cache_time = datetime.fromisoformat(cached_data['cached_at'])
            if datetime.now() - cache_time < cache_duration:
                print("ìºì‹œëœ í”„ë¡œê·¸ë¨ ë°ì´í„° ì‚¬ìš©")
                return cached_data['data']
        except Exception as e:
            print(f"ìºì‹œ ì½ê¸° ì˜¤ë¥˜: {e}")

    # ìƒˆë¡œ í¬ë¡¤ë§
    print("ğŸ”„ ìƒˆë¡œìš´ í”„ë¡œê·¸ë¨ ë°ì´í„° í¬ë¡¤ë§ ì¤‘...")
    crawler = BusanYouthProgramCrawler()
    programs = crawler.crawl_all_programs()

    # ìºì‹œ ì €ì¥
    cache_data = {
        'cached_at': datetime.now().isoformat(),
        'data': programs
    }

    try:
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        print("í”„ë¡œê·¸ë¨ ìºì‹œ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")

    return programs


def get_region_from_location(location, spaces_data=None):
    """ì¥ì†Œëª…ìœ¼ë¡œë¶€í„° ì§€ì—­ ì¶”ì¶œ"""
    if not location:
        return ""

    # ì²­ë…„ê³µê°„ ë°ì´í„°ì—ì„œ ë§¤ì¹­ ì‹œë„
    if spaces_data:
        for space in spaces_data:
            space_name = space.get('name', '')
            if location.strip() in space_name or space_name in location.strip():
                return space.get('region', '')

    # ì¥ì†Œëª…ì—ì„œ ì§ì ‘ ì§€ì—­ ì¶”ì¶œ ì‹œë„ (JSON ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ í™•ì¥)
    location_mappings = {
        # ê¸ˆì •êµ¬
        'ê¿ˆí„°': 'ê¸ˆì •êµ¬',
        'ì²­ë…„ì°½ì¡°ë°œì „ì†Œ   ê¿ˆí„°+': 'ê¸ˆì •êµ¬',
        'ì²­ë…„ì°½ì¡°ë°œì „ì†Œ ê¿ˆí„°': 'ê¸ˆì •êµ¬',
        'ê¸ˆì •': 'ê¸ˆì •êµ¬',

        # í•´ìš´ëŒ€êµ¬
        'í•´ìš´ëŒ€': 'í•´ìš´ëŒ€êµ¬',
        'í•´ìš´ëŒ€ ì²­ë…„ì±„ì›€ê³µê°„': 'í•´ìš´ëŒ€êµ¬',
        'í•´ìš´ëŒ€ ì²­ë…„JOBì¹´í˜': 'í•´ìš´ëŒ€êµ¬',

        # ë‚¨êµ¬
        'ê³ ê³ ì”½': 'ë‚¨êµ¬',
        'ì²­ë…„ì°½ì¡°ë°œì „ì†Œ  ê³ ê³ ì”½ Job': 'ë‚¨êµ¬',
        'ì²­ë…„ì°½ì¡°ë°œì „ì†Œ ê³ ê³ ì”½': 'ë‚¨êµ¬',
        'ë‚¨êµ¬': 'ë‚¨êµ¬',

        # ë™ë˜êµ¬
        'ë™ë˜': 'ë™ë˜êµ¬',
        'ë™ë˜êµ¬ ì²­ë…„ì–´ìš¸ë¦¼ì„¼í„°': 'ë™ë˜êµ¬',

        # ì¤‘êµ¬ (ì¶”ì •)
        'ì²­ë…„ì‘ë‹¹ì†Œ': 'ì¤‘êµ¬',
        'ì²­ë…„ë¬¸í™”êµë¥˜ê³µê°„': 'ì¤‘êµ¬',
        'ì²­ë…„ë¬¸í™”êµë¥˜ê³µê°„ \'ì²­ë…„ì‘ë‹¹ì†Œ\'': 'ì¤‘êµ¬',

        # ì§€ì—­ ë¯¸í™•ì • (ì¥ì†Œëª…ìœ¼ë¡œ ì¶”ì •)
        'ê³µê°„ìˆ²': 'ë¶€ì‚°ì§„êµ¬',  # ë™ë„¤ ì²­ë…„ê³µê°„ ê³µê°„ìˆ²
        'ë™ë„¤ ì²­ë…„ê³µê°„ ê³µê°„ìˆ²': 'ë¶€ì‚°ì§„êµ¬',

        # ê¸°íƒ€ ì§€ì—­ í‚¤ì›Œë“œ
        'ë¶€ì‚°ì§„': 'ë¶€ì‚°ì§„êµ¬',
        'ë¶êµ¬': 'ë¶êµ¬',
        'ì„œêµ¬': 'ì„œêµ¬',
        'ë™êµ¬': 'ë™êµ¬',
        'ì˜ë„': 'ì˜ë„êµ¬',
        'ì‚¬í•˜': 'ì‚¬í•˜êµ¬',
        'ê°•ì„œ': 'ê°•ì„œêµ¬',
        'ì—°ì œ': 'ì—°ì œêµ¬',
        'ìˆ˜ì˜': 'ìˆ˜ì˜êµ¬',
        'ì‚¬ìƒ': 'ì‚¬ìƒêµ¬',
        'ê¸°ì¥': 'ê¸°ì¥êµ°'
    }

    # ì™„ì „ ì¼ì¹˜ ìš°ì„  í™•ì¸
    if location.strip() in location_mappings:
        return location_mappings[location.strip()]

    # ë¶€ë¶„ ì¼ì¹˜ í™•ì¸
    for keyword, region in location_mappings.items():
        if keyword in location:
            return region

    return ""


def search_programs_by_region(region):
    """ì§€ì—­ë³„ ì²­ë…„ í”„ë¡œê·¸ë¨ ê²€ìƒ‰ (ë§ˆê°ì¼ ì„ë°• ìˆœ)"""
    from services.youth_space_crawler import get_youth_spaces_data

    programs = get_youth_programs_data()
    spaces_data = get_youth_spaces_data()  # ì²­ë…„ê³µê°„ ë°ì´í„°ë„ ê°€ì ¸ì˜¤ê¸°

    if not programs:
        return "í˜„ì¬ ì²­ë…„ í”„ë¡œê·¸ë¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    region_normalized = region.replace('êµ¬', '') if region.endswith('êµ¬') else region

    filtered_programs = []
    for program in programs:
        program_region = program.get('region', '')
        program_location = program.get('location', '')
        program_title = program.get('title', '')

        # 1. ì œëª©ì—ì„œ ì§€ì—­ í™•ì¸
        title_region_match = False
        if region_normalized in program_title or f"[{region}]" in program_title:
            title_region_match = True

        # 2. region í•„ë“œì—ì„œ ì§€ì—­ í™•ì¸
        region_field_match = region_normalized in program_region

        # 3. locationì—ì„œ ì§€ì—­ ì¶”ì¶œí•˜ì—¬ í™•ì¸
        location_region = get_region_from_location(program_location, spaces_data)
        location_region_match = region_normalized in location_region

        # í•˜ë‚˜ë¼ë„ ë§¤ì¹­ë˜ë©´ í¬í•¨
        if title_region_match or region_field_match or location_region_match:
            # ë§ˆê°ì¼ íŒŒì‹± ì¶”ê°€
            deadline = parse_deadline_date(program.get('application_period', ''))
            program['deadline_date'] = deadline

            # locationì—ì„œ ì¶”ì¶œí•œ ì§€ì—­ ì •ë³´ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
            if location_region and not program_region:
                program['region'] = location_region

            filtered_programs.append(program)

    if not filtered_programs:
        return f"**{region}**ì—ì„œ í˜„ì¬ ëª¨ì§‘ì¤‘ì¸ ì²­ë…„ ê³µê°„ í”„ë¡œê·¸ë¨ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ë§ˆê°ì¼ ì„ë°• ìˆœìœ¼ë¡œ ì •ë ¬ (ë§ˆê°ì¼ì´ ì—†ëŠ” ê²ƒì€ ë’¤ë¡œ)
    today = datetime.now()
    filtered_programs.sort(key=lambda x: (
        x['deadline_date'] is None,  # Noneì¸ ê²ƒë“¤ì„ ë’¤ë¡œ
        x['deadline_date'] if x['deadline_date'] else datetime.max  # ë§ˆê°ì¼ ì„ë°• ìˆœ
    ))

    result = f"**{region} ì²­ë…„ ê³µê°„ í”„ë¡œê·¸ë¨** ({len(filtered_programs)}ê°œ ëª¨ì§‘ì¤‘)\n"
    result += "ğŸ“… *ë§ˆê°ì¼ ì„ë°• ìˆœìœ¼ë¡œ ì •ë ¬ë˜ì—ˆìŠµë‹ˆë‹¤*\n\n"

    for program in filtered_programs[:8]:
        # ë§ˆê°ì¼ê¹Œì§€ ë‚¨ì€ ì¼ìˆ˜ ê³„ì‚°
        deadline_info = ""
        if program.get('deadline_date'):
            days_left = (program['deadline_date'] - today).days
            if days_left < 0:
                deadline_info = " âš ï¸ ë§ˆê°"
            elif days_left <= 3:
                deadline_info = f" ğŸ”¥ D-{days_left}"
            elif days_left <= 7:
                deadline_info = f" â° D-{days_left}"
            else:
                deadline_info = f" ğŸ“… D-{days_left}"

        result += format_program_info(program, deadline_info) + "\n"

    return result


def search_programs_by_keyword(keyword):
    """í‚¤ì›Œë“œë³„ ì²­ë…„ í”„ë¡œê·¸ë¨ ê²€ìƒ‰"""
    programs = get_youth_programs_data()
    if not programs:
        return "í˜„ì¬ ì²­ë…„ í”„ë¡œê·¸ë¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    filtered_programs = []
    keyword_lower = keyword.lower()

    for program in programs:
        if (keyword_lower in program.get('title', '').lower() or
                keyword_lower in program.get('location', '').lower() or
                keyword_lower in program.get('region', '').lower()):
            filtered_programs.append(program)

    if not filtered_programs:
        return f"**{keyword}** ê´€ë ¨ ëª¨ì§‘ì¤‘ì¸ ì²­ë…„ í”„ë¡œê·¸ë¨ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\në‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”!"

    result = f"ğŸ” **{keyword}** ê²€ìƒ‰ ê²°ê³¼ ({len(filtered_programs)}ê°œ ëª¨ì§‘ì¤‘)\n\n"

    for program in filtered_programs[:8]:  # ìµœëŒ€ 8ê°œ í‘œì‹œ
        result += format_program_info(program) + "\n"

    if len(filtered_programs) > 8:
        result += f"\n... ì™¸ {len(filtered_programs) - 8}ê°œ í”„ë¡œê·¸ë¨ ë” ìˆìŒ"

    return result


def format_program_info(program, deadline_info=""):
    """í”„ë¡œê·¸ë¨ ì •ë³´ í¬ë§·íŒ…"""
    result = f"**{program['title']}**{deadline_info}\n"

    if program.get('status'):
        status_emoji = "ğŸŸ¢" if program['status'] == 'ëª¨ì§‘ì¤‘' else "ğŸ”´"
        result += f"{status_emoji} {program['status']}\n"

    if program.get('application_period'):
        result += f"ğŸ“… ì‹ ì²­ê¸°ê°„: {program['application_period']}\n"

    if program.get('location'):
        result += f"ğŸ“ ì¥ì†Œ: {program['location']}\n"

    if program.get('region'):
        result += f"ğŸ›ï¸ ì§€ì—­: {program['region']}\n"

    if program.get('link'):
        result += f"ğŸ”— [ìì„¸íˆ ë³´ê¸°]({program['link']})\n"

    return result


def get_all_youth_programs():
    """ì „ì²´ ëª¨ì§‘ì¤‘ì¸ ì²­ë…„ í”„ë¡œê·¸ë¨ ëª©ë¡"""
    programs = get_youth_programs_data()
    if not programs:
        return "í˜„ì¬ ì²­ë…„ í”„ë¡œê·¸ë¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    result = f"**ë¶€ì‚° ì²­ë…„ í”„ë¡œê·¸ë¨ ëª¨ì§‘ì¤‘** ({len(programs)}ê°œ)\n\n"

    # ì§€ì—­ë³„ë¡œ ê·¸ë£¹í™”
    regions = {}
    for program in programs:
        region = program.get('region', 'ê¸°íƒ€')
        if not region or region == 'ê¸°íƒ€':
            # ì œëª©ì—ì„œ ì§€ì—­ ì¶”ì¶œ ì‹œë„
            title = program.get('title', '')
            region_match = re.search(r'\[([^]]+êµ¬)\]', title)
            if region_match:
                region = region_match.group(1)
            else:
                region = 'ì „ì²´/ê¸°íƒ€'

        if region not in regions:
            regions[region] = []
        regions[region].append(program)

    for region, region_programs in sorted(regions.items()):
        result += f"**{region}** ({len(region_programs)}ê°œ)\n"
        for program in region_programs[:3]:  # ê° ì§€ì—­ë‹¹ ìµœëŒ€ 3ê°œ
            result += f"{program['title']}\n"
            if program.get('application_period'):
                result += f"{program['application_period']}\n"

        if len(region_programs) > 3:
            result += f"     ... ì™¸ {len(region_programs) - 3}ê°œ ë”\n"
        result += "\n"

    result += "ğŸ’¡ ì§€ì—­ëª…ì´ë‚˜ í”„ë¡œê·¸ë¨ëª…ìœ¼ë¡œ ìì„¸í•œ ì •ë³´ë¥¼ ê²€ìƒ‰í•´ë³´ì„¸ìš”!"
    return result


def get_programs_by_category():
    """ì¹´í…Œê³ ë¦¬ë³„ í”„ë¡œê·¸ë¨ ë¶„ë¥˜"""
    programs = get_youth_programs_data()
    if not programs:
        return "í˜„ì¬ ì²­ë…„ í”„ë¡œê·¸ë¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    categories = {
        'ì·¨ì—…/ì§„ë¡œ': [],
        'êµìœ¡/ê°•ì˜': [],
        'ì°½ì—…': [],
        'ë¬¸í™”/ì˜ˆìˆ ': [],
        'ê¸°íƒ€': []
    }

    for program in programs:
        title = program.get('title', '').lower()
        categorized = False

        if any(keyword in title for keyword in ['ì·¨ì—…', 'job', 'ì»¨ì„¤íŒ…', 'ë©´ì ‘', 'ì´ë ¥ì„œ']):
            categories['ì·¨ì—…/ì§„ë¡œ'].append(program)
            categorized = True
        elif any(keyword in title for keyword in ['êµìœ¡', 'ê°•ì˜', 'ê³¼ì •', 'êµì‹¤', 'ìŠ¤ì¿¨']):
            categories['êµìœ¡/ê°•ì˜'].append(program)
            categorized = True
        elif any(keyword in title for keyword in ['ì°½ì—…', 'ì‚¬ì—…', 'ë¹„ì¦ˆë‹ˆìŠ¤']):
            categories['ì°½ì—…'].append(program)
            categorized = True
        elif any(keyword in title for keyword in ['ë¬¸í™”', 'ì˜ˆìˆ ', 'ê³µì—°', 'ì „ì‹œ', 'ìŒì•…', 'ë¯¸ìˆ ']):
            categories['ë¬¸í™”/ì˜ˆìˆ '].append(program)
            categorized = True

        if not categorized:
            categories['ê¸°íƒ€'].append(program)

    result = "**ì¹´í…Œê³ ë¦¬ë³„ ì²­ë…„ í”„ë¡œê·¸ë¨**\n\n"

    for category, category_programs in categories.items():
        if category_programs:
            result += f"**{category}** ({len(category_programs)}ê°œ)\n"
            for program in category_programs[:3]:
                result += f"{program['title']}\n"
            if len(category_programs) > 3:
                result += f"     ... ì™¸ {len(category_programs) - 3}ê°œ ë”\n"
            result += "\n"

    return result