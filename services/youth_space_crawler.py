import requests
import re
import time
import json
from bs4 import BeautifulSoup
from urllib.parse import urljoin, quote
from datetime import datetime


class BusanYouthSpaceCrawler:
    def __init__(self):
        self.base_url = "https://young.busan.go.kr"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.spaces_data = []

    def get_page_content(self, url, encoding='utf-8'):
        """í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            print(f"í˜ì´ì§€ ìš”ì²­: {url}")
            response = self.session.get(url, timeout=15)
            response.encoding = encoding

            if response.status_code == 200:
                return BeautifulSoup(response.content, 'html.parser')
            else:
                print(f"HTTP ì˜¤ë¥˜: {response.status_code}")
                return None

        except Exception as e:
            print(f"í˜ì´ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None

    def extract_space_info_from_li(self, li_element, order):
        """li.toggle_type ìš”ì†Œì—ì„œ ê³µê°„ ì •ë³´ ì¶”ì¶œ"""
        try:
            space_info = {
                'region': '',
                'name': '',
                'contact': '',
                'description': '',
                'address': '',
                'hours': '',
                'homepage': '',
                'sns': '',
                'rental_link': '',
                'program_link': ''
            }

            # 1. ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            plc_box = li_element.select_one('a.toggle .plc_box')
            if plc_box:
                # ì§€ì—­ ì •ë³´
                plc_gu = plc_box.select_one('.plc_gu')
                if plc_gu:
                    space_info['region'] = plc_gu.get_text(strip=True)

                # ê³µê°„ëª… ì¶”ì¶œ
                plc_tit = plc_box.select_one('.plc_tit')
                if plc_tit:
                    spans = plc_tit.find_all('span')
                    if len(spans) >= 2:
                        space_info['name'] = spans[1].get_text(strip=True)

                # ì—°ë½ì²˜
                plc_part = plc_box.select_one('.plc_part')
                if plc_part:
                    space_info['contact'] = plc_part.get_text(strip=True)

            # 2. ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            toggle_inner = li_element.select_one('.toggle_inner')
            if toggle_inner:
                # ê³µê°„ ì„¤ëª…
                spif_con = toggle_inner.select_one('.spif_con')
                if spif_con:
                    space_info['description'] = spif_con.get_text(strip=True)

                # ì£¼ì†Œ, ì´ìš©ì‹œê°„, ì—°ë½ì²˜ ì¶”ì¶œ
                arrow_list = toggle_inner.select_one('.arrow_list ul')
                if arrow_list:
                    li_items = arrow_list.find_all('li')
                    for li_item in li_items:
                        spans = li_item.find_all('span')
                        if len(spans) >= 2:
                            label = spans[0].get_text(strip=True)
                            value = spans[1].get_text(strip=True)

                            if 'ì£¼ì†Œ' in label:
                                space_info['address'] = value
                            elif 'ì´ìš©ì‹œê°„' in label or 'ìš´ì˜ì‹œê°„' in label:
                                space_info['hours'] = value
                            elif 'ì—°ë½ì²˜' in label:
                                if not space_info['contact']:
                                    space_info['contact'] = value

                # ë§í¬ ì •ë³´ ì¶”ì¶œ
                splink_list = toggle_inner.select('.splink_list a')
                for link in splink_list:
                    link_text = link.select_one('.splink_txt')
                    if link_text:
                        text = link_text.get_text(strip=True)
                        href = link.get('href', '')

                        if 'í™ˆí˜ì´ì§€' in text:
                            space_info['homepage'] = href
                        elif 'SNS' in text:
                            space_info['sns'] = href
                        elif 'ëŒ€ê´€' in text:
                            space_info['rental_link'] = href
                        elif 'í”„ë¡œê·¸ë¨' in text:
                            space_info['program_link'] = href

            # ìµœì†Œ ì¡°ê±´ í™•ì¸
            if space_info['name'] and space_info['region']:
                return space_info
            else:
                return None

        except Exception as e:
            print(f"li ìš”ì†Œ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None

    def extract_spaces_from_page(self, soup, page_num):
        """í˜ì´ì§€ì—ì„œ ì²­ë…„ê³µê°„ ëª©ë¡ ì¶”ì¶œ"""
        spaces = []
        space_list = soup.select('.policy_list.space_list ul li.toggle_type')

        for i, li_element in enumerate(space_list, 1):
            try:
                space_info = self.extract_space_info_from_li(li_element, i)
                if space_info:
                    spaces.append(space_info)
            except Exception as e:
                continue

        return spaces

    def has_space_content(self, soup):
        """í˜ì´ì§€ì— ê³µê°„ ì½˜í…ì¸ ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        if not soup:
            return False
        toggle_items = soup.select('.toggle_type')
        return len(toggle_items) > 0

    def crawl_all_spaces(self):
        """ëª¨ë“  ì²­ë…„ê³µê°„ í¬ë¡¤ë§"""
        print("ë¶€ì‚° ì²­ë…„ê³µê°„ í¬ë¡¤ë§ ì‹œì‘")
        all_spaces = []

        # 1, 2, 3 í˜ì´ì§€ í¬ë¡¤ë§
        for page in range(1, 4):
            print(f"í˜ì´ì§€ {page}/3 í¬ë¡¤ë§ ì¤‘...")

            if page == 1:
                url = "https://young.busan.go.kr/space/list.nm"
            else:
                possible_urls = [
                    f"https://young.busan.go.kr/space/list.nm?pageIndex={page}",
                    f"https://young.busan.go.kr/space/list.nm?page={page}",
                ]

                soup = None
                for url in possible_urls:
                    soup = self.get_page_content(url)
                    if soup and self.has_space_content(soup):
                        break
                    time.sleep(0.5)

                if not soup:
                    continue

            if page == 1:
                soup = self.get_page_content(url)

            if not soup:
                continue

            page_spaces = self.extract_spaces_from_page(soup, page)
            all_spaces.extend(page_spaces)
            time.sleep(1)  # í˜ì´ì§€ ê°„ ì§€ì—°

        print(f"í¬ë¡¤ë§ ì™„ë£Œ: {len(all_spaces)}ê°œ ê³µê°„ ìˆ˜ì§‘")
        self.spaces_data = all_spaces
        return all_spaces


def get_youth_spaces_data():
    """ì²­ë…„ê³µê°„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    import os
    from datetime import datetime, timedelta

    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ instance í´ë” ê²½ë¡œ ì„¤ì •
    basedir = os.path.abspath(os.path.dirname(__file__))
    project_root = os.path.dirname(basedir)  # servicesì˜ ìƒìœ„ í´ë” (í”„ë¡œì íŠ¸ ë£¨íŠ¸)
    instance_path = os.path.join(os.environ.get('RENDER_DISK_PATH', project_root), 'instance')
    if not os.path.exists(instance_path):
        os.makedirs(instance_path)

    cache_file = os.path.join(instance_path, 'youth_spaces_cache.json')
    cache_duration = timedelta(hours=24)  # 24ì‹œê°„ ìºì‹œ

    # ìºì‹œ íŒŒì¼ í™•ì¸
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)

            cache_time = datetime.fromisoformat(cached_data['cached_at'])
            if datetime.now() - cache_time < cache_duration:
                print("ìºì‹œëœ ì²­ë…„ê³µê°„ ë°ì´í„° ì‚¬ìš©")
                return cached_data['data']
        except Exception as e:
            print(f"ìºì‹œ ì½ê¸° ì˜¤ë¥˜: {e}")

    # ìƒˆë¡œ í¬ë¡¤ë§
    print("ğŸ”„ ìƒˆë¡œìš´ ì²­ë…„ê³µê°„ ë°ì´í„° í¬ë¡¤ë§ ì¤‘...")
    crawler = BusanYouthSpaceCrawler()
    spaces = crawler.crawl_all_spaces()

    # ìºì‹œ ì €ì¥
    cache_data = {
        'cached_at': datetime.now().isoformat(),
        'data': spaces
    }

    try:
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        print("ì²­ë…„ê³µê°„ ìºì‹œ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")

    return spaces


def search_spaces_by_region(region):
    """ì§€ì—­ë³„ ì²­ë…„ê³µê°„ ê²€ìƒ‰"""
    spaces = get_youth_spaces_data()
    if not spaces:
        return "í˜„ì¬ ì²­ë…„ê³µê°„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ì •í™•í•œ ì§€ì—­ ë§¤ì¹­ (ë¶€ë¶„ ë§¤ì¹­ â†’ ì •í™• ë§¤ì¹­ìœ¼ë¡œ ë³€ê²½)
    filtered_spaces = []
    for space in spaces:
        space_region = space.get('region', '').strip()
        if space_region == region:  # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ
            filtered_spaces.append(space)

    if not filtered_spaces:
        return f"**{region}**ì—ì„œ ì²­ë…„ê³µê°„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\në‹¤ë¥¸ ì§€ì—­ì„ ê²€ìƒ‰í•´ë³´ì„¸ìš”!"

    # í¬ë§· ìˆ˜ì •: ì¤„ë°”ê¿ˆ ì œê±°
    result = f"**{region} ì²­ë…„ê³µê°„({len(filtered_spaces)}ê°œ)**\n\n"

    for space in filtered_spaces[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
        result += format_space_info(space) + "\n"

    return result


def search_spaces_by_keyword(keyword):
    """í‚¤ì›Œë“œë³„ ì²­ë…„ê³µê°„ ê²€ìƒ‰"""
    spaces = get_youth_spaces_data()
    if not spaces:
        return "í˜„ì¬ ì²­ë…„ê³µê°„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    filtered_spaces = []
    keyword_lower = keyword.lower()

    for space in spaces:
        if (keyword_lower in space.get('name', '').lower() or
                keyword_lower in space.get('description', '').lower() or
                keyword_lower in space.get('region', '').lower()):
            filtered_spaces.append(space)

    if not filtered_spaces:
        return f"**{keyword}** ê´€ë ¨ ì²­ë…„ê³µê°„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\në‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”!"

    result = f"ğŸ” **{keyword}** ê²€ìƒ‰ ê²°ê³¼ ({len(filtered_spaces)}ê°œ)\n\n"

    for space in filtered_spaces[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
        result += format_space_info(space) + "\n"

    return result


def format_space_info(space):
    """ê³µê°„ ì •ë³´ í¬ë§·íŒ… - ì¤„ë°”ê¿ˆ ì œê±°"""
    # í¬ë§· ìˆ˜ì •: [ì§€ì—­]ì„ ê³µê°„ëª… ë°”ë¡œ ë’¤ì— ë¶™ì´ê¸°
    result = f"**{space['name']}[{space.get('region', '')}]**\n"

    if space.get('address'):
        result += f"ğŸ“ {space['address']}\n"
    if space.get('contact'):
        result += f"ğŸ“ {space['contact']}\n"
    if space.get('hours'):
        result += f"ğŸ•’ {space['hours']}\n"
    if space.get('description'):
        desc = space['description'][:100] + "..." if len(space['description']) > 100 else space['description']
        result += f"ğŸ“ {desc}\n"

    links = []
    if space.get('homepage'):
        links.append(f"[í™ˆí˜ì´ì§€]({space['homepage']})")
    if space.get('rental_link'):
        links.append(f"[ëŒ€ê´€ì‹ ì²­]({space['rental_link']})")
    if space.get('program_link'):
        links.append(f"[í”„ë¡œê·¸ë¨]({space['program_link']})")

    if links:
        result += f"ğŸ”— {' | '.join(links)}\n"

    return result


def get_all_youth_spaces():
    """ì „ì²´ ì²­ë…„ê³µê°„ ëª©ë¡"""
    spaces = get_youth_spaces_data()
    if not spaces:
        return "í˜„ì¬ ì²­ë…„ê³µê°„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    result = f"**ë¶€ì‚° ì²­ë…„ê³µê°„ ì „ì²´ ëª©ë¡** ({len(spaces)}ê°œ)\n\n"

    # ì§€ì—­ë³„ë¡œ ê·¸ë£¹í™”
    regions = {}
    for space in spaces:
        region = space.get('region', 'ê¸°íƒ€')
        if region not in regions:
            regions[region] = []
        regions[region].append(space['name'])

    for region, names in sorted(regions.items()):
        result += f"**ğŸ“ {region}** ({len(names)}ê°œ)\n"
        for name in names:
            result += f"  â€¢ {name}\n"
        result += "\n"

    result += "ğŸ’¡ ì§€ì—­ëª…ì´ë‚˜ ê³µê°„ëª…ìœ¼ë¡œ ìì„¸í•œ ì •ë³´ë¥¼ ê²€ìƒ‰í•´ë³´ì„¸ìš”!"
    return result
