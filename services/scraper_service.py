import requests
import re
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime


class BusanJobScraper:
    def __init__(self):
        self.base_url = "https://busanjob.net"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def get_job_list_urls(self):
        """ì±„ìš©ì •ë³´ í˜ì´ì§€ URLë“¤ì„ ìˆ˜ì§‘"""
        job_urls = [
            "/01_emif/emif01_1.asp?mbs=B",  # êµ¬ì¸ì •ë³´ (ê¸°ì—…ì±„ìš©)
            "/01_emif/emif01_1.asp?mbs=P",  # êµ¬ì¸ì •ë³´ (ê³µê³µì±„ìš©)
        ]
        return [urljoin(self.base_url, url) for url in job_urls]

    def scrape_job_listings(self, url, job_type, max_jobs=5):
        """íŠ¹ì • ì±„ìš© í˜ì´ì§€ì—ì„œ ì±„ìš©ê³µê³  ëª©ë¡ ìˆ˜ì§‘ (ìµœëŒ€ max_jobsê°œ)"""
        try:
            response = self.session.get(url, timeout=10)
            response.encoding = 'euc-kr'
            soup = BeautifulSoup(response.content, 'html.parser')

            jobs = []

            # ì±„ìš©ê³µê³  í…Œì´ë¸” ì°¾ê¸°
            tables = soup.find_all('table')

            for table in tables:
                rows = table.find_all('tr')

                for i, row in enumerate(rows):
                    if len(jobs) >= max_jobs:  # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
                        break

                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 3:
                        # ì²« ë²ˆì§¸ í–‰ì´ í—¤ë”ì¸ì§€ í™•ì¸
                        if i == 0 and any(keyword in cell.get_text() for cell in cells for keyword in
                                          ['ë²ˆí˜¸', 'ì œëª©', 'íšŒì‚¬', 'ë“±ë¡ì¼', 'ë§ˆê°ì¼']):
                            continue

                        job_info = self.extract_job_info_from_row(cells, job_type, url)
                        if job_info and job_info['title'] and len(job_info['title']) > 2:
                            jobs.append(job_info)

                if len(jobs) >= max_jobs:
                    break

            # ë§ˆê°ì¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ê°€ê¹Œìš´ ìˆœì„œëŒ€ë¡œ)
            jobs_with_dates = []
            jobs_without_dates = []

            for job in jobs:
                deadline_date = self.parse_deadline_date(job.get('deadline', ''))
                if deadline_date:
                    job['deadline_date'] = deadline_date
                    jobs_with_dates.append(job)
                else:
                    jobs_without_dates.append(job)

            # ë§ˆê°ì¼ì´ ìˆëŠ” ê²ƒì„ ìš°ì„ ìœ¼ë¡œ ì •ë ¬
            jobs_with_dates.sort(key=lambda x: x['deadline_date'])

            return jobs_with_dates + jobs_without_dates

        except Exception as e:
            print(f"Error scraping {url}: {str(e)}")
            return []

    def extract_job_info_from_row(self, cells, job_type, base_url):
        """í…Œì´ë¸” í–‰ì—ì„œ ì±„ìš©ì •ë³´ ì¶”ì¶œ"""
        try:
            job_info = {
                'job_type': job_type,
                'company': '',
                'title': '',
                'location': '',
                'deadline': '',
                'register_date': '',
                'link': '',
                'details': ''
            }

            # ì…€ ë‚´ìš© ì¶”ì¶œ
            cell_texts = [cell.get_text(strip=True) for cell in cells]

            if len(cell_texts) >= 3:
                for i, text in enumerate(cell_texts):
                    if not text or text.isdigit():
                        continue

                    if not job_info['title'] and len(text) > 2:
                        job_info['title'] = text
                        # ì œëª©ì—ì„œ ë§í¬ ì°¾ê¸°
                        link_element = cells[i].find('a')
                        if link_element and link_element.get('href'):
                            href = link_element['href']
                            if href.startswith('http'):
                                job_info['link'] = href
                            else:
                                job_info['link'] = urljoin(base_url, href)
                    elif not job_info['company'] and job_info['title'] and len(text) > 1:
                        job_info['company'] = text
                    elif self.is_date_format(text):
                        if not job_info['register_date']:
                            job_info['register_date'] = text
                        elif not job_info['deadline']:
                            job_info['deadline'] = text

                job_info['details'] = ' | '.join([t for t in cell_texts if t and not t.isdigit()])

                return job_info if job_info['title'] else None

        except Exception as e:
            print(f"Error extracting job info from row: {str(e)}")
            return None

    def is_date_format(self, text):
        """í…ìŠ¤íŠ¸ê°€ ë‚ ì§œ í˜•ì‹ì¸ì§€ í™•ì¸"""
        if not text:
            return False
        return bool(re.search(r'\d{4}[-./]\d{1,2}[-./]\d{1,2}|\d{1,2}[-./]\d{1,2}|\d{4}ë…„|\d{1,2}ì›”', text))

    def parse_deadline_date(self, deadline_str):
        """ë§ˆê°ì¼ ë¬¸ìì—´ì„ ë‚ ì§œ ê°ì²´ë¡œ ë³€í™˜"""
        if not deadline_str:
            return None

        try:
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
            date_patterns = [
                r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})',
                r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼?',
                r'(\d{1,2})[-./](\d{1,2})'
            ]

            for pattern in date_patterns:
                match = re.search(pattern, deadline_str)
                if match:
                    groups = match.groups()
                    if len(groups) == 3:
                        year, month, day = groups
                        if len(year) == 2:
                            year = '20' + year
                        return datetime(int(year), int(month), int(day)).date()
                    elif len(groups) == 2:
                        month, day = groups
                        current_year = datetime.now().year
                        return datetime(current_year, int(month), int(day)).date()

            return None
        except:
            return None

    def get_latest_jobs(self, max_jobs=3):
        """ìµœì‹  ì±„ìš©ê³µê³ ë¥¼ ë§ˆê°ì¼ ìˆœìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°"""
        all_jobs = []
        job_urls = self.get_job_list_urls()

        for url in job_urls:
            job_type = "ê¸°ì—…ì±„ìš©" if "mbs=B" in url else "ê³µê³µì±„ìš©"
            jobs = self.scrape_job_listings(url, job_type, max_jobs)
            all_jobs.extend(jobs)
            time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

        # ì „ì²´ ê²°ê³¼ì—ì„œ ë§ˆê°ì¼ ê¸°ì¤€ ì •ë ¬ í›„ ìƒìœ„ max_jobsê°œ ë°˜í™˜
        jobs_with_dates = [job for job in all_jobs if job.get('deadline_date')]
        jobs_without_dates = [job for job in all_jobs if not job.get('deadline_date')]

        jobs_with_dates.sort(key=lambda x: x['deadline_date'])

        final_jobs = (jobs_with_dates + jobs_without_dates)[:max_jobs]
        return final_jobs


def get_busanjob_latest_jobs():
    """ë¶€ì‚°ì¡ì—ì„œ ìµœì‹  ì±„ìš©ì •ë³´ 3ê°œë¥¼ ê°€ì ¸ì™€ì„œ í¬ë§·íŒ…"""
    try:
        scraper = BusanJobScraper()
        jobs = scraper.get_latest_jobs(max_jobs=3)

        if not jobs:
            return "í˜„ì¬ ë¶€ì‚°ì¡ì—ì„œ ì±„ìš©ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

        result_text = "**Busan Jobs**ì—ì„œ ê°€ì ¸ì˜¨ ìµœì‹  ì±„ìš©ì •ë³´ (ë§ˆê° ì„ë°• ìˆœ)ì…ë‹ˆë‹¤! ğŸ”¥\n\n"

        for i, job in enumerate(jobs, 1):
            result_text += f"**{i}. {job['title']}**\n"
            if job.get('company'):
                result_text += f"   - **íšŒì‚¬:** {job['company']}\n"
            result_text += f"   - **ìœ í˜•:** {job['job_type']}\n"
            if job.get('deadline'):
                result_text += f"   - **ë§ˆê°ì¼:** {job['deadline']}\n"
            if job.get('register_date'):
                result_text += f"   - **ë“±ë¡ì¼:** {job['register_date']}\n"
            if job.get('link'):
                result_text += f"   - **ìƒì„¸ë³´ê¸°:** [ë§í¬ ë°”ë¡œê°€ê¸°]({job['link']})\n"
            result_text += "\n"

        result_text += "ë” ë§ì€ ì±„ìš©ì •ë³´ëŠ” [ë¶€ì‚°ì¡ í™ˆí˜ì´ì§€](https://busanjob.net)ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        return result_text

    except Exception as e:
        print(f"ë¶€ì‚°ì¡ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë¶€ì‚°ì¡ ì‚¬ì´íŠ¸ì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì§ì ‘ [ë¶€ì‚°ì¡ ì‚¬ì´íŠ¸](https://busanjob.net)ë¥¼ ë°©ë¬¸í•´ ì£¼ì„¸ìš”."