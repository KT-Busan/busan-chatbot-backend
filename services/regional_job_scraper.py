import requests
import re
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, quote
from datetime import datetime


class RegionalJobScraper:
    def __init__(self):
        self.base_url = "https://busanjob.net"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def get_region_jobs(self, region, job_type="ê¸°ì—…", max_jobs=3):
        """íŠ¹ì • ì§€ì—­ì˜ ì±„ìš©ê³µê³ ë¥¼ ìˆ˜ì§‘"""
        try:
            # URL êµ¬ì„± - job_typeì— ë”°ë¼ ë‹¤ë¥¸ URL ì‚¬ìš©
            if job_type == "ê³µê³µ":
                search_url = f"https://busanjob.net/01_emif/emif01_1.asp?keyword={quote(region)}&region="
            else:  # ê¸°ì—… ì±„ìš©
                search_url = f"https://busanjob.net/01_emif/emif01.asp?keyword={quote(region)}&region="

            response = self.session.get(search_url, timeout=10)
            response.encoding = 'euc-kr'
            soup = BeautifulSoup(response.content, 'html.parser')

            jobs = []

            # ì±„ìš©ê³µê³  í…Œì´ë¸” ì°¾ê¸°
            tables = soup.find_all('table')

            for table in tables:
                rows = table.find_all('tr')

                for i, row in enumerate(rows):
                    if len(jobs) >= max_jobs:
                        break

                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 4:  # ìµœì†Œ 4ê°œ ì»¬ëŸ¼ (ë²ˆí˜¸, ì œëª©, íšŒì‚¬, ê¸°íƒ€ ì •ë³´)
                        # ì²« ë²ˆì§¸ í–‰ì´ í—¤ë”ì¸ì§€ í™•ì¸
                        if i == 0 and any(keyword in cell.get_text() for cell in cells for keyword in
                                          ['ë²ˆí˜¸', 'ì œëª©', 'íšŒì‚¬', 'ë“±ë¡ì¼', 'ë§ˆê°ì¼', 'êµ¬ë¶„']):
                            continue

                        job_info = self.extract_job_info_from_row(cells, search_url, region)
                        if job_info and job_info['title'] and len(job_info['title']) > 2:
                            # í˜„ì¬ ëª¨ì§‘ ì¤‘ì¸ ê³µê³ ë§Œ í•„í„°ë§í•˜ê³  ì§€ì—­ëª…ì´ í¬í•¨ëœ ì±„ìš©ê³µê³ ë§Œ í•„í„°ë§
                            if self.is_currently_recruiting(job_info) and (
                                    region in job_info['title'] or
                                    region in job_info.get('company', '') or
                                    region in job_info.get('location', '')
                            ):
                                job_info['job_type'] = job_type  # job_type ì •ë³´ ì¶”ê°€
                                jobs.append(job_info)

                if len(jobs) >= max_jobs:
                    break

            # ë“±ë¡ì¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹  ìˆœ)
            jobs_with_dates = []
            jobs_without_dates = []

            for job in jobs:
                register_date = self.parse_date(job.get('register_date', ''))
                if register_date:
                    job['register_date_obj'] = register_date
                    jobs_with_dates.append(job)
                else:
                    jobs_without_dates.append(job)

            # ë“±ë¡ì¼ì´ ìµœì‹ ì¸ ê²ƒì„ ìš°ì„ ìœ¼ë¡œ ì •ë ¬
            jobs_with_dates.sort(key=lambda x: x['register_date_obj'], reverse=True)

            final_jobs = (jobs_with_dates + jobs_without_dates)[:max_jobs]

            # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ì¼ë°˜ ì±„ìš©ê³µê³ ë„ í¬í•¨
            if len(final_jobs) < max_jobs:
                general_jobs = self.get_general_jobs(max_jobs - len(final_jobs), job_type)
                final_jobs.extend(general_jobs)

            return final_jobs[:max_jobs]

        except Exception as e:
            print(f"Error scraping regional jobs for {region}: {str(e)}")
            return []

    def get_general_jobs(self, max_jobs=3, job_type="ê¸°ì—…"):
        """ì¼ë°˜ ì±„ìš©ê³µê³  ìˆ˜ì§‘ (ì§€ì—­ ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•  ë•Œ ì‚¬ìš©)"""
        try:
            # job_typeì— ë”°ë¼ ë‹¤ë¥¸ URL ì‚¬ìš©
            if job_type == "ê³µê³µ":
                url = "https://busanjob.net/01_emif/emif01_1.asp"
            else:  # ê¸°ì—… ì±„ìš©
                url = "https://busanjob.net/01_emif/emif01.asp"

            response = self.session.get(url, timeout=10)
            response.encoding = 'euc-kr'
            soup = BeautifulSoup(response.content, 'html.parser')

            jobs = []
            tables = soup.find_all('table')

            for table in tables:
                rows = table.find_all('tr')

                for i, row in enumerate(rows):
                    if len(jobs) >= max_jobs:
                        break

                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 4:
                        if i == 0:  # í—¤ë” ìŠ¤í‚µ
                            continue

                        job_info = self.extract_job_info_from_row(cells, url)
                        if job_info and job_info['title'] and len(job_info['title']) > 2:
                            # í˜„ì¬ ëª¨ì§‘ ì¤‘ì¸ ê³µê³ ë§Œ í•„í„°ë§
                            if self.is_currently_recruiting(job_info):
                                job_info['job_type'] = job_type  # job_type ì •ë³´ ì¶”ê°€
                                jobs.append(job_info)

                if len(jobs) >= max_jobs:
                    break

            return jobs

        except Exception as e:
            print(f"Error scraping general jobs: {str(e)}")
            return []

    def extract_job_info_from_row(self, cells, base_url, region=None):
        """í…Œì´ë¸” í–‰ì—ì„œ ì±„ìš©ì •ë³´ ì¶”ì¶œ"""
        try:
            job_info = {
                'title': '',
                'company': '',
                'location': '',
                'job_type': '',
                'deadline': '',
                'register_date': '',
                'link': '',
                'details': '',
                'region': region or ''
            }

            # ì…€ ë‚´ìš© ì¶”ì¶œ
            cell_texts = [cell.get_text(strip=True) for cell in cells]

            if len(cell_texts) >= 3:
                for i, text in enumerate(cell_texts):
                    if not text or text.isdigit():
                        continue

                    # ì œëª© ì°¾ê¸° (ë§í¬ê°€ ìˆëŠ” ì…€ì—ì„œ ìš°ì„  ì¶”ì¶œ)
                    if not job_info['title'] and len(text) > 3:
                        link_element = cells[i].find('a')
                        if link_element:
                            job_info['title'] = text
                            href = link_element.get('href')
                            if href:
                                if href.startswith('http'):
                                    job_info['link'] = href
                                else:
                                    job_info['link'] = urljoin(base_url, href)
                        elif len(text) > 5 and not job_info['title']:
                            job_info['title'] = text

                    # íšŒì‚¬ëª… ì°¾ê¸°
                    elif not job_info['company'] and job_info['title'] and len(text) > 1:
                        if text != job_info['title'] and not self.is_date_format(text):
                            job_info['company'] = text

                    # ë‚ ì§œ ì •ë³´ ì°¾ê¸°
                    elif self.is_date_format(text):
                        if not job_info['register_date']:
                            job_info['register_date'] = text
                        elif not job_info['deadline']:
                            job_info['deadline'] = text

                    # ìœ„ì¹˜ ì •ë³´ ì°¾ê¸° (ë¶€ì‚° ì§€ì—­ëª… í¬í•¨)
                    elif any(area in text for area in ['êµ¬', 'êµ°', 'ë¶€ì‚°', 'í•´ìš´ëŒ€', 'ê°•ì„œ', 'ê¸°ì¥']):
                        if not job_info['location']:
                            job_info['location'] = text

                # ì„¸ë¶€ ì •ë³´ëŠ” ëª¨ë“  ì…€ì˜ í…ìŠ¤íŠ¸ ì¡°í•©
                job_info['details'] = ' | '.join([t for t in cell_texts if t and not t.isdigit() and len(t) > 1])

                return job_info if job_info['title'] else None

        except Exception as e:
            print(f"Error extracting job info from row: {str(e)}")
            return None

    def parse_date(self, date_str):
        """ë‚ ì§œ íŒŒì‹± í•¨ìˆ˜ (ìˆ˜ì •ë¨)"""
        if not date_str:
            return None

        try:
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
            date_patterns = [
                r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})',
                r'(\d{2})[-./](\d{1,2})[-./](\d{1,2})',
                r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼?',
                r'(\d{1,2})[-./](\d{1,2})'
            ]

            for pattern in date_patterns:
                match = re.search(pattern, date_str)
                if match:
                    groups = match.groups()
                    if len(groups) == 3:
                        year, month, day = groups
                        # 2ìë¦¬ ì—°ë„ë¥¼ 4ìë¦¬ë¡œ ë³€í™˜
                        if len(year) == 2:
                            if int(year) >= 25:  # 25ë…„ ì´ìƒì´ë©´ 2025ë…„ ì´í›„
                                year = '20' + year
                            else:
                                year = '20' + year
                        return datetime(int(year), int(month), int(day)).date()
                    elif len(groups) == 2:
                        month, day = groups
                        current_year = datetime.now().year
                        return datetime(current_year, int(month), int(day)).date()

            return None
        except Exception as e:
            print(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e} - ì…ë ¥ê°’: {date_str}")
            return None

    def is_date_format(self, text):
        """í…ìŠ¤íŠ¸ê°€ ë‚ ì§œ í˜•ì‹ì¸ì§€ í™•ì¸ (ìˆ˜ì •ë¨)"""
        if not text:
            return False
        return bool(re.search(
            r'\d{4}[-./]\d{1,2}[-./]\d{1,2}|\d{2}[-./]\d{1,2}[-./]\d{1,2}|\d{1,2}[-./]\d{1,2}|\d{4}ë…„|\d{1,2}ì›”', text))

    def is_currently_recruiting(self, job_info):
        """í˜„ì¬ ëª¨ì§‘ ì¤‘ì¸ ê³µê³ ì¸ì§€ í™•ì¸"""
        from datetime import date

        # ë§ˆê°ì¼ì´ ìˆìœ¼ë©´ í˜„ì¬ ë‚ ì§œì™€ ë¹„êµ
        if job_info.get('deadline'):
            deadline_date = self.parse_date(job_info['deadline'])
            if deadline_date:
                return deadline_date >= date.today()

        # ì œëª©ì—ì„œ ëª¨ì§‘ ìƒíƒœ í™•ì¸
        title = job_info.get('title', '').lower()
        if any(keyword in title for keyword in ['ë§ˆê°', 'ì¢…ë£Œ', 'ì™„ë£Œ']):
            return False

        # ëª¨ì§‘ì¤‘, ì ‘ìˆ˜ì¤‘ ë“±ì˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ëª¨ì§‘ ì¤‘
        if any(keyword in title for keyword in ['ëª¨ì§‘ì¤‘', 'ì ‘ìˆ˜ì¤‘', 'ì±„ìš©ì¤‘', 'recruiting']):
            return True

        # ë“±ë¡ì¼ì´ 30ì¼ ì´ë‚´ë©´ í˜„ì¬ ëª¨ì§‘ ì¤‘ìœ¼ë¡œ ê°„ì£¼
        if job_info.get('register_date'):
            register_date = self.parse_date(job_info['register_date'])
            if register_date:
                from datetime import timedelta
                return (date.today() - register_date).days <= 30

        # ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ì§‘ ì¤‘ìœ¼ë¡œ ê°„ì£¼ (ìµœì‹  ì •ë³´ì¼ ê°€ëŠ¥ì„±)
        return True

    def extract_overseas_job_info(self, cells, base_url, country=None):
        """í•´ì™¸ ì±„ìš© í…Œì´ë¸” í–‰ì—ì„œ ì±„ìš©ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        try:
            job_info = {
                'title': '',
                'company': '',
                'location': '',
                'job_type': '',
                'deadline': '',
                'register_date': '',
                'link': '',
                'details': '',
                'country': country or ''
            }

            # ì…€ ë‚´ìš© ì¶”ì¶œ
            cell_texts = [cell.get_text(strip=True) for cell in cells]

            if len(cell_texts) >= 3:
                # ì œëª©ê³¼ ë§í¬ ì°¾ê¸° (ì²« ë²ˆì§¸ ìš°ì„ ìˆœìœ„)
                for i, cell in enumerate(cells):
                    link_element = cell.find('a')
                    if link_element and not job_info['title']:
                        title_text = cell.get_text(strip=True)
                        if len(title_text) > 3:
                            job_info['title'] = title_text
                            href = link_element.get('href')
                            if href:
                                if href.startswith('http'):
                                    job_info['link'] = href
                                else:
                                    job_info['link'] = urljoin(base_url, href)
                            break

                # ë‚˜ë¨¸ì§€ ì •ë³´ ì¶”ì¶œ
                for i, text in enumerate(cell_texts):
                    if not text or text.isdigit() or text == job_info.get('title', ''):
                        continue

                    # íšŒì‚¬ëª… ì°¾ê¸°
                    if not job_info['company'] and job_info['title'] and len(text) > 1:
                        if not self.is_date_format(text) and not any(
                                keyword in text.lower() for keyword in ['ë§ˆê°', 'ëª¨ì§‘', 'ì±„ìš©']):
                            job_info['company'] = text

                    # ë‚ ì§œ ì •ë³´ ì°¾ê¸°
                    elif self.is_date_format(text):
                        if '~' in text or 'ë§ˆê°' in text or 'deadline' in text.lower():
                            job_info['deadline'] = text
                        elif not job_info['register_date']:
                            job_info['register_date'] = text

                    # êµ­ê°€/ìœ„ì¹˜ ì •ë³´ ì°¾ê¸° (ìš°ì„ ìˆœìœ„: ì…ë ¥ëœ êµ­ê°€ëª… í¬í•¨)
                    elif country and country.lower() in text.lower():
                        job_info['country'] = text
                    elif any(keyword in text.lower() for keyword in
                             ['ë¯¸êµ­', 'ì¼ë³¸', 'ì¤‘êµ­', 'ë² íŠ¸ë‚¨', 'ì‹±ê°€í¬ë¥´', 'ë§ë ˆì´ì‹œì•„', 'usa', 'japan', 'china', 'vietnam', 'singapore',
                              'malaysia']):
                        if not job_info['location']:
                            job_info['location'] = text

                    # ì§ì¢… ì •ë³´ ì°¾ê¸° (ë” í¬ê´„ì )
                    elif any(keyword in text.lower() for keyword in
                             ['ê°œë°œì', 'ì—”ì§€ë‹ˆì–´', 'ë§¤ë‹ˆì €', 'ë””ìì´ë„ˆ', 'ë§ˆì¼€íŒ…', 'ì˜ì—…', 'ê´€ë¦¬', 'êµì‚¬', 'ê°„í˜¸ì‚¬', 'ìš”ë¦¬ì‚¬', 'developer',
                              'engineer', 'manager', 'designer']):
                        if not job_info['job_type']:
                            job_info['job_type'] = text

                # êµ­ê°€ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì…ë ¥ëœ êµ­ê°€ëª… ì‚¬ìš©
                if not job_info['country'] and not job_info['location']:
                    job_info['country'] = country

                # ì„¸ë¶€ ì •ë³´ëŠ” ì¤‘ìš”í•œ ì…€ë“¤ë§Œ ì¡°í•©
                important_texts = [t for t in cell_texts if
                                   t and not t.isdigit() and len(t) > 1 and t != job_info.get('title', '')]
                job_info['details'] = ' | '.join(important_texts[:3])  # ì²˜ìŒ 3ê°œë§Œ

                return job_info if job_info['title'] else None

        except Exception as e:
            print(f"Error extracting overseas job info from row: {str(e)}")
            return None

    def extract_overseas_job_from_table(self, cells, base_url, target_country):
        """ë¶€ì‚°ì¡ í•´ì™¸ì±„ìš© í…Œì´ë¸”ì—ì„œ ì •ë³´ ì¶”ì¶œ (ê³µê³ ëª…/ê¸°ê´€ëª…, êµ­ê°€, ì§ì¢…, ë§ˆê°ì¼)"""
        try:
            if len(cells) < 4:
                return None

            # ê° ì…€ì—ì„œ ì •ë³´ ì¶”ì¶œ
            title_cell = cells[0]  # ê³µê³ ëª…/ê¸°ê´€ëª…
            country_cell = cells[1]  # êµ­ê°€
            job_type_cell = cells[2]  # ì§ì¢…
            deadline_cell = cells[3]  # ë§ˆê°ì¼

            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            title = title_cell.get_text(strip=True)
            country = country_cell.get_text(strip=True)
            job_type = job_type_cell.get_text(strip=True)
            deadline = deadline_cell.get_text(strip=True)

            # ë§í¬ ì¶”ì¶œ (ë³´í†µ ì œëª© ì…€ì— ìˆìŒ)
            link = base_url
            link_element = title_cell.find('a')
            if link_element and link_element.get('href'):
                href = link_element['href']
                if href.startswith('http'):
                    link = href
                else:
                    link = urljoin(base_url, href)

            # ìœ íš¨í•œ ë°ì´í„°ì¸ì§€ í™•ì¸
            if not title or len(title) < 3:
                return None

            job_info = {
                'title': title,
                'country': country,
                'job_type': job_type,
                'deadline': deadline,
                'link': link
            }

            print(f"ğŸ“‹ ì¶”ì¶œëœ ì±„ìš©ì •ë³´: {title} | {country} | {job_type} | {deadline}")
            return job_info

        except Exception as e:
            print(f"í…Œì´ë¸” ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None

    def is_target_country_job(self, job_info, target_country):
        """í•´ë‹¹ êµ­ê°€ì˜ ì±„ìš©ê³µê³ ì¸ì§€ í™•ì¸"""
        target_lower = target_country.lower()

        # ì œëª©ì—ì„œ í™•ì¸
        title = job_info.get('title', '').lower()
        if target_lower in title:
            return True

        # êµ­ê°€ í•„ë“œì—ì„œ í™•ì¸
        country = job_info.get('country', '').lower()
        if target_lower in country:
            return True

        # ì˜ì–´-í•œêµ­ì–´ ë§¤í•‘ í™•ì¸
        country_mapping = {
            'ì¼ë³¸': ['japan', 'japanese', 'ì¼ë³¸'],
            'ë¯¸êµ­': ['usa', 'america', 'american', 'ë¯¸êµ­'],
            'ì¤‘êµ­': ['china', 'chinese', 'ì¤‘êµ­'],
            'ë…ì¼': ['germany', 'german', 'ë…ì¼'],
            'ë² íŠ¸ë‚¨': ['vietnam', 'vietnamese', 'ë² íŠ¸ë‚¨'],
            'ì‹±ê°€í¬ë¥´': ['singapore', 'ì‹±ê°€í¬ë¥´'],
            'ë§ë ˆì´ì‹œì•„': ['malaysia', 'ë§ë ˆì´ì‹œì•„'],
            'ìºë‚˜ë‹¤': ['canada', 'ìºë‚˜ë‹¤'],
            'í˜¸ì£¼': ['australia', 'í˜¸ì£¼'],
            'í´ë€ë“œ': ['poland', 'polish', 'í´ë€ë“œ'],
            'ì¸ë„': ['india', 'indian', 'ì¸ë„'],
            'íƒœêµ­': ['thailand', 'thai', 'íƒœêµ­']
        }

        if target_lower in country_mapping:
            keywords = country_mapping[target_lower]
            for keyword in keywords:
                if keyword in title or keyword in country:
                    return True

        return False

    def get_overseas_jobs_with_keyword(self, country):
        """í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ í•´ì™¸ ì±„ìš©ì •ë³´ ìˆ˜ì§‘"""
        try:
            url = f"https://busanjob.net/01_emif/emif01_2.asp?keyword={quote(country)}"
            response = self.session.get(url, timeout=10)
            response.encoding = 'euc-kr'
            soup = BeautifulSoup(response.content, 'html.parser')

            jobs = []

            # í…Œì´ë¸”ì—ì„œ ì •ë³´ ì¶”ì¶œ ì‹œë„
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 3:
                        row_text = ' '.join([cell.get_text(strip=True) for cell in cells])
                        if self.contains_country_simple(row_text, country):
                            job_info = self.extract_job_from_row_text(cells, url)
                            if job_info:
                                jobs.append(job_info)
                                if len(jobs) >= 3:
                                    return jobs

            return jobs
        except Exception as e:
            print(f"í‚¤ì›Œë“œ ê²€ìƒ‰ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return []

    def get_overseas_jobs_from_text(self, country):
        """í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ"""
        try:
            url = "https://busanjob.net/01_emif/emif01_2.asp"
            response = self.session.get(url, timeout=10)
            response.encoding = 'euc-kr'

            # í˜ì´ì§€ì— êµ­ê°€ëª…ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            content = response.text
            if country not in content and country.lower() not in content.lower():
                return []

            # ê¸°ë³¸ ì±„ìš©ì •ë³´ ìƒì„± (ì‹¤ì œ í¬ë¡¤ë§ì´ ì–´ë ¤ìš¸ ë•Œì˜ ëŒ€ì•ˆ)
            jobs = [
                {
                    'title': f'{country} í˜„ì§€ ê¸°ì—… ì±„ìš©',
                    'job_type': 'ë‹¤ì–‘í•œ ì§ì¢…',
                    'deadline': 'ìƒì‹œ ëª¨ì§‘',
                    'link': url,
                    'period': 'ìˆ˜ì‹œ'
                },
                {
                    'title': f'{country} í•œêµ­ê³„ ê¸°ì—… ì±„ìš©',
                    'job_type': 'ê´€ë¦¬ì§, ê¸°ìˆ ì§',
                    'deadline': 'ì±„ìš©ì‹œê¹Œì§€',
                    'link': url,
                    'period': 'ì—°ì¤‘'
                }
            ]

            return jobs
        except Exception as e:
            print(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []

    def contains_country_simple(self, text, country):
        """í…ìŠ¤íŠ¸ì— í•´ë‹¹ êµ­ê°€ëª…ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê°„ë‹¨íˆ í™•ì¸"""
        text_lower = text.lower()
        country_lower = country.lower()

        # ì§ì ‘ ë§¤ì¹­
        if country_lower in text_lower:
            return True

        # ì˜ì–´-í•œêµ­ì–´ ë§¤í•‘
        country_maps = {
            'ì¼ë³¸': ['japan', 'japanese'],
            'ë¯¸êµ­': ['usa', 'america', 'american'],
            'ì¤‘êµ­': ['china', 'chinese'],
            'ë…ì¼': ['germany', 'german'],
            'ë² íŠ¸ë‚¨': ['vietnam', 'vietnamese'],
            'ì‹±ê°€í¬ë¥´': ['singapore'],
            'ë§ë ˆì´ì‹œì•„': ['malaysia'],
            'ìºë‚˜ë‹¤': ['canada'],
            'í˜¸ì£¼': ['australia', 'australian']
        }

        if country_lower in country_maps:
            for keyword in country_maps[country_lower]:
                if keyword in text_lower:
                    return True

        return False

    def extract_job_type_from_text(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì§ì¢… ì •ë³´ ì¶”ì¶œ"""
        job_keywords = [
            'ê°œë°œì', 'ì—”ì§€ë‹ˆì–´', 'ë§¤ë‹ˆì €', 'ë””ìì´ë„ˆ', 'ë§ˆì¼€íŒ…', 'ì˜ì—…',
            'ê´€ë¦¬', 'êµì‚¬', 'ê°„í˜¸ì‚¬', 'ìš”ë¦¬ì‚¬', 'ê¸°ìˆ ì', 'ìƒë‹´ì›',
            'developer', 'engineer', 'manager', 'designer', 'marketing'
        ]

        text_lower = text.lower()
        for keyword in job_keywords:
            if keyword in text_lower:
                return keyword

        return 'ë‹¤ì–‘í•œ ì§ì¢…'

    def extract_date_from_text(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ"""
        import re

        # ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
        date_patterns = [
            r'\d{4}[-./]\d{1,2}[-./]\d{1,2}',
            r'\d{1,2}[-./]\d{1,2}',
            r'\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼?'
        ]

        for pattern in date_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group()

        # ìƒëŒ€ì  í‘œí˜„ ì°¾ê¸°
        if any(keyword in text for keyword in ['ìƒì‹œ', 'ìˆ˜ì‹œ', 'ì±„ìš©ì‹œê¹Œì§€', 'ì—°ì¤‘']):
            return 'ìƒì‹œ ëª¨ì§‘'

        return 'ì±„ìš©ì‹œê¹Œì§€'

    def extract_job_from_row_text(self, cells, base_url):
        """í–‰ ë°ì´í„°ì—ì„œ ì±„ìš©ì •ë³´ ì¶”ì¶œ"""
        try:
            cell_texts = [cell.get_text(strip=True) for cell in cells if cell.get_text(strip=True)]

            if len(cell_texts) < 2:
                return None

            # ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
            title = max(cell_texts, key=len) if cell_texts else ''

            # ë§í¬ ì°¾ê¸°
            link = base_url
            for cell in cells:
                link_element = cell.find('a')
                if link_element and link_element.get('href'):
                    link = urljoin(base_url, link_element['href'])
                    break

            return {
                'title': title,
                'job_type': self.extract_job_type_from_text(title),
                'deadline': self.extract_date_from_text(' '.join(cell_texts)),
                'link': link,
                'period': ''
            }
        except Exception as e:
            print(f"í–‰ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None

    def contains_country(self, job_info, target_country):
        """ì±„ìš©ê³µê³ ê°€ ëŒ€ìƒ êµ­ê°€ì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸"""
        target_lower = target_country.lower()

        # ì œëª©ì—ì„œ êµ­ê°€ëª… í™•ì¸
        title = job_info.get('title', '').lower()
        if target_lower in title:
            return True

        # êµ­ê°€ í•„ë“œì—ì„œ í™•ì¸
        country = job_info.get('country', '').lower()
        if target_lower in country:
            return True

        # ì˜ì–´-í•œêµ­ì–´ ë§¤í•‘ í™•ì¸
        country_mapping = {
            'ì¼ë³¸': ['japan', 'japanese', 'ì¼ë³¸'],
            'ë¯¸êµ­': ['usa', 'america', 'american', 'ë¯¸êµ­'],
            'ì¤‘êµ­': ['china', 'chinese', 'ì¤‘êµ­'],
            'ë…ì¼': ['germany', 'german', 'ë…ì¼'],
            'ë² íŠ¸ë‚¨': ['vietnam', 'vietnamese', 'ë² íŠ¸ë‚¨'],
            'ì‹±ê°€í¬ë¥´': ['singapore', 'ì‹±ê°€í¬ë¥´'],
            'ë§ë ˆì´ì‹œì•„': ['malaysia', 'ë§ë ˆì´ì‹œì•„'],
            'ìºë‚˜ë‹¤': ['canada', 'ìºë‚˜ë‹¤'],
            'í˜¸ì£¼': ['australia', 'í˜¸ì£¼']
        }

        if target_lower in country_mapping:
            keywords = country_mapping[target_lower]
            for keyword in keywords:
                if keyword in title or keyword in country:
                    return True

        return False


def get_regional_jobs(region, job_type="ê¸°ì—…"):
    """ì§€ì—­ë³„ ì±„ìš©ì •ë³´ë¥¼ ê°€ì ¸ì™€ì„œ í¬ë§·íŒ…"""
    try:
        scraper = RegionalJobScraper()
        jobs = scraper.get_region_jobs(region, job_type, max_jobs=3)

        if not jobs:
            return f"í˜„ì¬ **{region}**ì—ì„œ ëª¨ì§‘ ì¤‘ì¸ **{job_type} ì±„ìš©** ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\në” ë§ì€ ì±„ìš©ì •ë³´ëŠ” [ë¶€ì‚°ì¡ ì±„ìš©ì •ë³´](https://busanjob.net/01_emif/emif01.asp)ì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”."

        result_text = f"ğŸ“ **{region} {job_type} ì±„ìš©ì •ë³´** (í˜„ì¬ ëª¨ì§‘ ì¤‘ì¸ ìƒìœ„ 3ê°œ)ì…ë‹ˆë‹¤!\n\n"

        for i, job in enumerate(jobs, 1):
            result_text += f"**{i}. {job['title']}**\n"
            if job.get('company'):
                result_text += f"   - **íšŒì‚¬:** {job['company']}\n"
            if job.get('location'):
                result_text += f"   - **ìœ„ì¹˜:** {job['location']}\n"
            if job.get('register_date'):
                result_text += f"   - **ë“±ë¡ì¼:** {job['register_date']}\n"
            if job.get('deadline'):
                result_text += f"   - **ë§ˆê°ì¼:** {job['deadline']}\n"
            if job.get('link'):
                result_text += f"   - **ìƒì„¸ë³´ê¸°:** [ë§í¬ ë°”ë¡œê°€ê¸°]({job['link']})\n"
            result_text += "\n"

        result_text += f"ë” ë§ì€ {region} ì±„ìš©ì •ë³´ëŠ” [ë¶€ì‚°ì¡ ì±„ìš©ì •ë³´](https://busanjob.net/01_emif/emif01.asp?keyword={region})ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        return result_text

    except Exception as e:
        print(f"ì§€ì—­ë³„ ì±„ìš©ì •ë³´ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ **{region} {job_type} ì±„ìš©** ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì§ì ‘ [ë¶€ì‚°ì¡ ì‚¬ì´íŠ¸](https://busanjob.net/01_emif/emif01.asp)ë¥¼ ë°©ë¬¸í•´ ì£¼ì„¸ìš”."


def get_overseas_jobs(country):
    """í•´ì™¸ ì±„ìš©ì •ë³´ë¥¼ ê°€ì ¸ì™€ì„œ í¬ë§·íŒ… - ë¶€ì‚°ì¡ í…Œì´ë¸” êµ¬ì¡° ê¸°ë°˜"""
    try:
        scraper = RegionalJobScraper()

        # ë¶€ì‚°ì¡ í•´ì™¸ì±„ìš© í˜ì´ì§€ ì ‘ì†
        url = "https://busanjob.net/01_emif/emif01_2.asp"
        response = scraper.session.get(url, timeout=10)
        response.encoding = 'euc-kr'
        soup = BeautifulSoup(response.content, 'html.parser')

        jobs = []

        # ì±„ìš©ì •ë³´ í…Œì´ë¸” ì°¾ê¸° (ê³µê³ ëª…/ê¸°ê´€ëª…, êµ­ê°€, ì§ì¢…, ë§ˆê°ì¼ êµ¬ì¡°)
        tables = soup.find_all('table')

        for table in tables:
            rows = table.find_all('tr')

            # í—¤ë” í–‰ ì°¾ê¸°
            header_found = False
            for i, row in enumerate(rows):
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 4:
                    header_text = ' '.join([cell.get_text(strip=True) for cell in cells])

                    # í…Œì´ë¸” í—¤ë” í™•ì¸ (ê³µê³ ëª…, êµ­ê°€, ì§ì¢…, ë§ˆê°ì¼)
                    if any(keyword in header_text for keyword in ['ê³µê³ ëª…', 'ê¸°ê´€ëª…', 'êµ­ê°€', 'ì§ì¢…', 'ë§ˆê°ì¼']):
                        header_found = True
                        print(f"âœ… í•´ì™¸ì±„ìš© í…Œì´ë¸” í—¤ë” ë°œê²¬: {header_text}")

                        # í—¤ë” ë‹¤ìŒ í–‰ë“¤ë¶€í„° ë°ì´í„° ì¶”ì¶œ
                        for j in range(i + 1, len(rows)):
                            data_row = rows[j]
                            data_cells = data_row.find_all(['td', 'th'])

                            if len(data_cells) >= 4:
                                job_info = scraper.extract_overseas_job_from_table(data_cells, url, country)
                                if job_info:
                                    jobs.append(job_info)
                        break

            if header_found:
                break

        # ì…ë ¥í•œ êµ­ê°€ì™€ ê´€ë ¨ëœ ì±„ìš©ê³µê³ ë§Œ í•„í„°ë§
        filtered_jobs = []
        for job in jobs:
            if scraper.is_target_country_job(job, country):
                filtered_jobs.append(job)

        # ë§ˆê°ì¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ê°€ê¹Œìš´ ìˆœ)
        from datetime import date
        today = date.today()

        current_jobs = []
        for job in filtered_jobs:
            deadline_date = scraper.parse_date(job.get('deadline', ''))
            if deadline_date and deadline_date >= today:
                job['deadline_date_obj'] = deadline_date
                current_jobs.append(job)
            elif not deadline_date:  # ë§ˆê°ì¼ ì •ë³´ê°€ ì—†ìœ¼ë©´ í¬í•¨
                current_jobs.append(job)

        # ë§ˆê°ì¼ ìˆœ ì •ë ¬
        jobs_with_deadline = [job for job in current_jobs if job.get('deadline_date_obj')]
        jobs_without_deadline = [job for job in current_jobs if not job.get('deadline_date_obj')]

        jobs_with_deadline.sort(key=lambda x: x['deadline_date_obj'])
        final_jobs = (jobs_with_deadline + jobs_without_deadline)[:3]

        if not final_jobs:
            return f"í˜„ì¬ **{country}**ì—ì„œ ëª¨ì§‘ ì¤‘ì¸ í•´ì™¸ ì±„ìš© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\në” ë§ì€ í•´ì™¸ ì±„ìš©ì •ë³´ëŠ” [ë¶€ì‚°ì¡ í•´ì™¸ì±„ìš©](https://busanjob.net/01_emif/emif01_2.asp)ì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”."

        result_text = f"ğŸŒ **{country} í•´ì™¸ ì±„ìš©ì •ë³´** (ë§ˆê° ì„ë°• ìˆœ {len(final_jobs)}ê°œ)ì…ë‹ˆë‹¤!\n\n"

        for i, job in enumerate(final_jobs, 1):
            result_text += f"**{i}. {job['title']}**\n"
            if job.get('job_type'):
                result_text += f"   - **ì§ì¢…:** {job['job_type']}\n"
            if job.get('deadline'):
                result_text += f"   - **ë§ˆê°ì¼:** {job['deadline']}\n"
            if job.get('link'):
                result_text += f"   - **ìƒì„¸ë³´ê¸°:** [ë§í¬ ë°”ë¡œê°€ê¸°]({job['link']})\n"
            result_text += "\n"

        result_text += f"ë” ë§ì€ {country} í•´ì™¸ ì±„ìš©ì •ë³´ëŠ” [ë¶€ì‚°ì¡ í•´ì™¸ì±„ìš©](https://busanjob.net/01_emif/emif01_2.asp)ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        return result_text

    except Exception as e:
        print(f"í•´ì™¸ ì±„ìš©ì •ë³´ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ **{country} í•´ì™¸ ì±„ìš©** ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì§ì ‘ [ë¶€ì‚°ì¡ í•´ì™¸ì±„ìš©](https://busanjob.net/01_emif/emif01_2.asp)ì„ ë°©ë¬¸í•´ ì£¼ì„¸ìš”."